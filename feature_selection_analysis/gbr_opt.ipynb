{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from skopt import gp_minimize\n",
    "\n",
    "import utils.dev_config as dev_conf\n",
    "import utils.preprocessing as prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = dev_conf.get_dev_directories(\"../dev_paths.txt\")\n",
    "unified_dsets = [\"unified_cervical_data\", \"unified_uterine_data\", \"unified_uterine_endometrial_data\"]\n",
    "matrisome_list = f\"{dirs.data_dir}/matrisome/matrisome_hs_masterlist.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrisome_df = prep.load_matrisome_df(matrisome_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand = np.random.RandomState()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and filter survival data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_code = {\"Alive\": 0, \"Dead\": 1}\n",
    "covariate_cols = [\"figo_stage\", \"age_at_diagnosis\", \"race\", \"ethnicity\"]\n",
    "dep_cols = [\"vital_status\", \"survival_time\"]\n",
    "cat_cols = [\"race\", \"ethnicity\", \"figo_chr\"]\n",
    "survival_df = prep.load_survival_df(f\"{dirs.data_dir}/{unified_dsets[i]}/survival_data.tsv\", event_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_survival_df = (\n",
    "    prep.decode_figo_stage(survival_df[[\"sample_name\"] + dep_cols + covariate_cols].dropna(), to=\"c\")\n",
    "        .query(\"vital_status == 1\")\n",
    "        .drop([\"vital_status\"], axis=1)\n",
    "        .pipe(pd.get_dummies, columns=cat_cols)\n",
    "        .reset_index(drop = True)\n",
    ")\n",
    "filtered_survival_df.columns = filtered_survival_df.columns.str.replace(' ', '_')\n",
    "\n",
    "print(filtered_survival_df.shape)\n",
    "# filtered_survival_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load normalized matrisome count data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_matrisome_counts_df = pd.read_csv(f\"{dirs.data_dir}/{unified_dsets[i]}/norm_matrisome_counts.tsv\", sep='\\t')\n",
    "norm_filtered_matrisome_counts_t_df = prep.transpose_df(\n",
    "    norm_matrisome_counts_df[[\"geneID\"] + list(filtered_survival_df.sample_name)], \"geneID\", \"sample_name\"\n",
    ")\n",
    "print(norm_filtered_matrisome_counts_t_df.shape)\n",
    "# norm_filtered_matrisome_counts_t_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join survival and count data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = (\n",
    "    pd.merge(filtered_survival_df, norm_filtered_matrisome_counts_t_df, on=\"sample_name\")\n",
    "        .set_index(\"sample_name\")\n",
    ")\n",
    "print(joined_df.shape)\n",
    "# joined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand.seed(123)\n",
    "shuffled_df = joined_df.sample(frac=1, random_state=rand)\n",
    "x_df = shuffled_df.iloc[:, 1:]\n",
    "y_df = shuffled_df.iloc[:, [0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(h_params, X, y):\n",
    "    print(h_params)\n",
    "    model = GradientBoostingRegressor(\n",
    "        loss=h_params[0],\n",
    "        learning_rate=h_params[1],\n",
    "        n_estimators=h_params[2],\n",
    "        max_depth=h_params[3],\n",
    "        max_features=h_params[4],\n",
    "        alpha=h_params[5]\n",
    "    )\n",
    "    # Standardize all variables (except one-hots)\n",
    "    # Don't really need to do this for decision tree methods, but might as well\n",
    "    # repeat the procedure that we're doing for the SVMs, just for posterity\n",
    "    pipeline = make_pipeline(\n",
    "        ColumnTransformer([\n",
    "            (\"standard\", StandardScaler(), [\"age_at_diagnosis\"] + list(norm_filtered_matrisome_counts_t_df.columns[1:]))\n",
    "        ], remainder=\"passthrough\"),\n",
    "        model\n",
    "    )\n",
    "    ttr = TransformedTargetRegressor(regressor=pipeline, transformer=StandardScaler())\n",
    "    return -np.mean(cross_val_score(\n",
    "        ttr,\n",
    "        X,\n",
    "        y,\n",
    "        cv=KFold(n_splits=5),\n",
    "        n_jobs=-1,\n",
    "        scoring=\"neg_mean_absolute_error\"\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = [\n",
    "    # We skip \"ls\" since we're optimizing for mean absolute error in the h_param opt.\n",
    "    Categorical([\"lad\", \"huber\", \"quantile\"], name=\"loss\"),\n",
    "    Real(1e-3, 1e-1, name=\"learning_rate\"),\n",
    "    Integer(int(1e2), int(1e3), name=\"n_estimators\"),\n",
    "    Integer(2, 5, name=\"max_depth\"),\n",
    "    Categorical([\"auto\", \"sqrt\", \"log2\"], name=\"max_features\"),\n",
    "    Real(0.5, 0.9, name=\"alpha\"),\n",
    "]\n",
    "n_initial = 20 * (len(space[0].categories) + len(space[4].categories))\n",
    "n_calls = 100 * (len(space[0].categories) + len(space[4].categories))\n",
    "# n_initial = 1 * (len(space[0].categories) + len(space[4].categories))\n",
    "# n_calls = 1 * (len(space[0].categories) + len(space[4].categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_callback(res):\n",
    "    # Does this model correspond with the best score seen so far?\n",
    "    if res.func_vals[-1] == res.fun:\n",
    "        h_param_df = pd.DataFrame({\n",
    "            \"param\": res.space.dimension_names + [\"loss_achieved\"],\n",
    "            \"param_value\": res.x + [res.fun]\n",
    "        })\n",
    "        h_param_df.to_csv(\"opt_gbr_h_params.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = gp_minimize(\n",
    "    lambda h_ps: objective(h_ps, x_df, y_df),\n",
    "    space,\n",
    "    verbose=True,\n",
    "    random_state=rand,\n",
    "    n_initial_points=n_initial,\n",
    "    n_calls = n_calls,\n",
    "    n_jobs=-1,\n",
    "    callback=save_callback\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
