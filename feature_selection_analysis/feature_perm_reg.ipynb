{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import perf_counter\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, explained_variance_score\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import utils.dev_config as dev_conf\n",
    "import utils.preprocessing as prep\n",
    "import utils.optimization as opt\n",
    "import utils.feature_selection as feat_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = dev_conf.get_dev_directories(\"../dev_paths.txt\")\n",
    "unified_dsets = [\"unified_cervical_data\", \"unified_uterine_data\", \"unified_uterine_endometrial_data\"]\n",
    "matrisome_list = f\"{dirs.data_dir}/matrisome/matrisome_hs_masterlist.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrisome_df = prep.load_matrisome_df(matrisome_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 123\n",
    "rand = np.random.RandomState()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_performance(estimator, x, y, random_state):\n",
    "    train_size = int(x.shape[0] * 2/3)\n",
    "    test_size = x.shape[0] - train_size\n",
    "    train_idx = random_state.choice(x.shape[0], size=train_size, replace=False)\n",
    "    train_mask = np.in1d(np.arange(x.shape[0]), train_idx)\n",
    "    \n",
    "    estimator.fit(x[train_mask], y[train_mask])\n",
    "    yhat = estimator.predict(x[~train_mask])\n",
    "    \n",
    "    pred_df = pd.DataFrame({\"y\": np.squeeze(y[~train_mask].values), \"yhat\": np.squeeze(yhat)})\n",
    "    pred_df[\"obs\"] = pred_df.index\n",
    "    pred_df = pd.melt(pred_df, id_vars=[\"obs\"], value_vars=[\"y\", \"yhat\"], var_name=\"y_type\", value_name=\"y_val\")\n",
    "    \n",
    "    ys = list(zip(np.squeeze(y[~train_mask].values), np.squeeze(yhat)))\n",
    "    xs = list(range(test_size))\n",
    "    \n",
    "    _, ax = plt.subplots(figsize=(10, 6))\n",
    "    sns.scatterplot(data=pred_df, x=\"obs\", y=\"y_val\", hue=\"y_type\", ax=ax)\n",
    "    ax.plot((xs, xs), ([i for (i, j) in ys], [j for (i, j) in ys]), c=\"black\", alpha=0.2)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and filter survival data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_code = {\"Alive\": 0, \"Dead\": 1}\n",
    "covariate_cols = [\"figo_stage\", \"age_at_diagnosis\", \"race\", \"ethnicity\"]\n",
    "dep_cols = [\"vital_status\", \"survival_time\"]\n",
    "cat_cols = [\"race\", \"ethnicity\", \"figo_chr\"]\n",
    "survival_df = prep.load_survival_df(f\"{dirs.data_dir}/{unified_dsets[i]}/survival_data.tsv\", event_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_survival_df = (\n",
    "    prep.decode_figo_stage(survival_df[[\"sample_name\"] + dep_cols + covariate_cols].dropna(), to=\"c\")\n",
    "        .query(\"vital_status == 1\")\n",
    "        .drop([\"vital_status\"], axis=1)\n",
    "        .pipe(pd.get_dummies, columns=cat_cols)\n",
    "        .reset_index(drop = True)\n",
    ")\n",
    "filtered_survival_df.columns = filtered_survival_df.columns.str.replace(' ', '_')\n",
    "\n",
    "print(filtered_survival_df.shape)\n",
    "# filtered_survival_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load normalized matrisome count data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_matrisome_counts_df = pd.read_csv(f\"{dirs.data_dir}/{unified_dsets[i]}/norm_matrisome_counts.tsv\", sep='\\t')\n",
    "norm_filtered_matrisome_counts_t_df = prep.transpose_df(\n",
    "    norm_matrisome_counts_df[[\"geneID\"] + list(filtered_survival_df.sample_name)], \"geneID\", \"sample_name\"\n",
    ")\n",
    "print(norm_filtered_matrisome_counts_t_df.shape)\n",
    "# norm_filtered_matrisome_counts_t_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join survival and count data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = (\n",
    "    pd.merge(filtered_survival_df, norm_filtered_matrisome_counts_t_df, on=\"sample_name\")\n",
    "        .set_index(\"sample_name\")\n",
    ")\n",
    "print(joined_df.shape)\n",
    "# joined_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create models from saved optimizer results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVR\n",
    "Need to do feature permutation importance, since we use a kernel method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_h_param_df = pd.read_csv(\"opt_svr_h_params.tsv\", sep=\"\\t\")\n",
    "svr_kwargs = pd.Series(svr_h_param_df.param_value[:-1].values, index=svr_h_param_df.param[:-1]).to_dict()\n",
    "svr = SVR(\n",
    "    kernel=svr_kwargs[\"kernel\"],\n",
    "    C=float(svr_kwargs[\"C\"]),\n",
    "    epsilon=float(svr_kwargs[\"epsilon\"]),\n",
    "    gamma=float(svr_kwargs[\"gamma\"]),\n",
    "    degree=int(svr_kwargs[\"degree\"]),\n",
    "    coef0=float(svr_kwargs[\"coef0\"])\n",
    ")\n",
    "svr_pipeline = make_pipeline(\n",
    "    ColumnTransformer([\n",
    "        (\"standard\", StandardScaler(), [\"age_at_diagnosis\"] + list(norm_filtered_matrisome_counts_t_df.columns[1:]))\n",
    "    ], remainder=\"passthrough\"),\n",
    "    svr\n",
    ")\n",
    "svr_ttr = TransformedTargetRegressor(regressor=svr_pipeline, transformer=StandardScaler())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosted Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr_h_param_df = pd.read_csv(\"opt_gbr_h_params.tsv\", sep=\"\\t\")\n",
    "gbr_kwargs = pd.Series(gbr_h_param_df.param_value[:-1].values, index=gbr_h_param_df.param[:-1]).to_dict()\n",
    "gbr = GradientBoostingRegressor(\n",
    "    loss=gbr_kwargs[\"loss\"],\n",
    "    learning_rate=float(gbr_kwargs[\"learning_rate\"]),\n",
    "    n_estimators=int(gbr_kwargs[\"n_estimators\"]),\n",
    "    max_depth=int(gbr_kwargs[\"max_depth\"]),\n",
    "    max_features=gbr_kwargs[\"max_features\"],\n",
    "    alpha=float(gbr_kwargs[\"alpha\"]),\n",
    "    random_state=rand\n",
    ")\n",
    "\n",
    "gbr_pipeline = make_pipeline(\n",
    "    ColumnTransformer([\n",
    "        (\"standard\", StandardScaler(), [\"age_at_diagnosis\"] + list(norm_filtered_matrisome_counts_t_df.columns[1:]))\n",
    "    ], remainder=\"passthrough\"),\n",
    "    gbr\n",
    ")\n",
    "gbr_ttr = TransformedTargetRegressor(regressor=gbr_pipeline, transformer=StandardScaler())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect cross validated feature permutation results\n",
    "$baseline - perm\\_score > 0 \\Longrightarrow \\text{feature permutation reduces performance} \\Longrightarrow \\text{feature is important}$\n",
    "\n",
    "EX:\n",
    "\n",
    "$baseline\\_neg\\_MAE = -500, \\; perm\\_neg\\_MAE = -510$\n",
    "\n",
    "$\\Longrightarrow -500 -(-510) = 10 > 0 \\Longrightarrow perm\\_importance > 0 \\Longrightarrow \\text{feature is important}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand.seed(seed)\n",
    "x_df, y_df = prep.shuffle_data(joined_df, rand)\n",
    "svr_perm_res, svr_ref_scores = opt.cv_permutation_importance(svr_ttr, x_df, y_df, \"neg_mean_absolute_error\", random_state=rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_perm_imp = np.concatenate([\n",
    "    (r.importances) for r in svr_perm_res\n",
    "], axis=1)\n",
    "svr_imp_meds = np.median(svr_perm_imp, axis=1)\n",
    "svr_imp_means = np.mean(svr_perm_imp, axis=1)\n",
    "\n",
    "# Assuming each feature was equally important, we would expect them to influence the reference metric by this value\n",
    "svr_ref_score_mean = svr_ref_scores.mean()\n",
    "svr_ref_score_med = np.median(svr_ref_scores)\n",
    "svr_naive_ref_score = -mean_absolute_error(y_df.values, np.repeat(np.median(y_df.values), y_df.shape[0])[:, np.newaxis])\n",
    "(svr_ref_score_mean - svr_naive_ref_score) / np.abs(svr_naive_ref_score) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = vis_performance(svr_ttr, x_df, y_df, rand)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_perm_res_df = feat_sel.gather_perm_res(x_df, svr_imp_means, svr_ref_score_mean, norm_filtered_matrisome_counts_t_df.columns[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand.seed(seed)\n",
    "x_df, y_df = prep.shuffle_data(joined_df, rand)\n",
    "gbr_perm_res, gbr_ref_scores = opt.cv_permutation_importance(gbr_ttr, x_df, y_df, \"neg_mean_absolute_error\", random_state=rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr_perm_imp = np.concatenate([\n",
    "    (r.importances) for r in gbr_perm_res\n",
    "], axis=1)\n",
    "gbr_imp_meds = np.median(gbr_perm_imp, axis=1)\n",
    "gbr_imp_means = np.mean(gbr_perm_imp, axis=1)\n",
    "\n",
    "# Assuming each feature was equally important, we would expect them to influence the reference metric by this value\n",
    "gbr_ref_score_mean = gbr_ref_scores.mean()\n",
    "gbr_ref_score_med = np.median(gbr_ref_scores)\n",
    "gbr_naive_ref_score = -mean_absolute_error(y_df.values, np.repeat(np.median(y_df.values), y_df.shape[0])[:, np.newaxis])\n",
    "(gbr_ref_score_mean - gbr_naive_ref_score) / np.abs(gbr_naive_ref_score) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = vis_performance(gbr_ttr, x_df, y_df, rand)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr_perm_res_df = feat_sel.gather_perm_res(x_df, gbr_imp_means, gbr_ref_score_mean, norm_filtered_matrisome_counts_t_df.columns[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GBR seems to react much more strongly to feature permutation than SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_perm_res_df.sort_values(\"mean_imp\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr_perm_res_df.sort_values(\"mean_imp\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_perm_res_df.to_csv(f\"{dirs.analysis_dir}/{unified_dsets[i]}_svr_permutation_results.tsv\", sep=\"\\t\", index=False)\n",
    "gbr_perm_res_df.to_csv(f\"{dirs.analysis_dir}/{unified_dsets[i]}_gbr_permutation_results.tsv\", sep=\"\\t\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
